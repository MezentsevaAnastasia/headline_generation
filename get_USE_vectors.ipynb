{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "get_USE_vectors",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVjNK8shFKOC"
      },
      "source": [
        "%%capture\n",
        "#@title Setup Environment\n",
        "# Install the latest Tensorflow version.\n",
        "!pip install tensorflow_text\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULWTRSawcxIC",
        "outputId": "4df4e579-609b-4e59-d0bf-6e186e4f2a5e"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSeY-MUQo2Ha"
      },
      "source": [
        "#@title Setup common imports and functions\n",
        "import bokeh\n",
        "import bokeh.models\n",
        "import bokeh.plotting\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from simpleneighbors import SimpleNeighbors\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk2IRjZFGDsK"
      },
      "source": [
        "This is additional boilerplate code where we import the pre-trained ML model we will use to encode text throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkmF3w8WGLcM"
      },
      "source": [
        "# The 16-language multilingual module is the default but feel free\n",
        "# to pick others from the list and compare the results.\n",
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3' #@param ['https://tfhub.dev/google/universal-sentence-encoder-multilingual/3', 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3']\n",
        "\n",
        "model = hub.load(module_url)\n",
        "\n",
        "def embed_text(input):\n",
        "  return model(input)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPy6ZvnUevA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c89647d-7468-45e4-bd35-b7e23354eb34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzZgAiHeSseA",
        "outputId": "7b1d4e89-590a-448f-bfe6-091eed2ce413"
      },
      "source": [
        "import json\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence_vectors = list()\n",
        "with open('/content/drive/MyDrive/gis/ria_1k.json', 'rb') as f:\n",
        "  for line in tqdm(f):\n",
        "    document = json.loads(line)\n",
        "    clean_text = re.sub('(<[^<]+?>|&nbsp;|&ndash;|  )', ' ', document['text'].lower())\n",
        "    result_text = clean_text.strip()\n",
        "    sentences = nltk.sent_tokenize(result_text, language=\"russian\")\n",
        "    vectors = embed_text(sentences)\n",
        "    sentence_vectors.append(np.array(vectors))\n",
        "    sentences = list()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10it [00:00, 98.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000it [00:09, 102.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I2Kv64ke10H"
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/gis/vectors.pkl', 'wb') as f:\n",
        "  pickle.dump(sentence_vectors,f) "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWtQ4JjAE9QE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}